<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>
</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Creative and Descriptive Paper Title." />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px"> [AAAI 2024] Aleth-NeRF: Illumination Adaptive NeRF <br> with Concealing Field Assumption 
</span>
		<br/>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://cuiziteng.github.io/">Ziteng Cui<sup>1,2</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://sites.google.com/view/linguedu/home">Lin Gu<sup>3,1</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://jimmysuen.github.io/">Xiao Sun<sup>2*</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://maxianzheng.github.io/">Xianzheng Ma<sup>4</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="http://mmlab.siat.ac.cn/yuqiao/">Yu Qiao<sup>2</sup></a></span>
						</center>
					</td>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada<sup>1,3</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=1000px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:14px"><a>1. The University of Tokyo</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:14px"><a>2. Shanghai AI Laboratory</a></span><br>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:14px"><a>3. RIKEN AIP</a></span><br>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:14px"><a>4. University of Oxford</a></span><br>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=600px>
				<tr>
					<td align=center width=170px>
						<center>
							<span style="font-size:18px"><a href='https://arxiv.org/abs/2312.09093'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=170px>
						<center>
							<span style="font-size:18px"><a href='https://github.com/cuiziteng/Aelth-NeRF'>[GitHub]</a></span><br>
						</center>
					</td>
					<td align=center width=170px>
						<center>
							<span style="font-size:18px"><a href='https://drive.google.com/file/d/1YBSHMSFmHxAHHS9qo_qdthchhl8IlKCP/view?usp=share_link'>[LOM Dataset]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<br/>
	<center>
		<table align=center width=850px>
			<tr>
				<td width=360px>
					<center>
						<img class="round" style="width:800px" src="resources/buu-min.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
	
	<hr>

	<table align=center width=950px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				The standard Neural Radiance Fields (NeRF) paradigm employs a viewer-centered methodology, entangling the aspects of illumination and material 
				reflectance into emission solely from 3D points. This simplified rendering approach presents challenges in accurately modeling images captured 
				under adverse lighting conditions, such as low light or over-exposure. Motivated by the ancient Greek emission theory that posits visual perception 
				as a result of rays emanating from the eyes, we slightly refine the conventional NeRF framework to train NeRF under challenging light conditions and 
				generate normal-light condition novel views unsupervised. We introduce the concept of a ”Concealing Field,” which assigns transmittance values to the 
				surrounding air to account for illumination effects. In dark scenarios, we assume that object emissions maintain a standard lighting level but are 
				attenuated as they traverse the air during the rendering process. Concealing Field thus compel NeRF to learn reasonable density and colour estimations for 
				objects even in dimly lit situations. Similarly, the Concealing Field can mitigate over-exposed emissions during the rendering stage. 
				Furthermore, we present a comprehensive multi-view dataset captured under challenging illumination conditions for evaluation. 
			</td>
		</tr>
	</table>
	<br>
	
	<table align=center width=1000px>
				<tr>
					<td align=center width=250px>
						<center>
							<span style="font-size:16px">NeRF</span><br>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							 <span style="font-size:16px">Image Enhance[1] + NeRF</span><br>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							 <span style="font-size:16px">Video Enhance[2] + NeRF</span><br>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							 <span style="font-size:16px">Aleth-NeRF</span><br>
						</center>
					</td>
				</tr>
			</table>
	
	
	
	<table align=center width=1000px>
				<tr>
					<td align=center width=250px>
						<center>
							<video width="240" height="140"  muted autoplay loop>
							 <source src="resources/buu/images_nerf.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							 <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/buu/images_IAT.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							  <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/buu/images_LLVE.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							  <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/buu/images_aleth.mp4" type="video/mp4">
							</video>
						</center>
					</td>
				</tr>
			</table>
	
	<table align=center width=1000px>
				<tr>
					<td align=center width=250px>
						<center>
							<video width="240" height="140"  muted autoplay loop>
							 <source src="resources/shurb/images_nerf.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							 <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/shurb/images_IAT.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							  <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/shurb/images_LLVE.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							  <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/shurb/images_aleth.mp4" type="video/mp4">
							</video>
						</center>
					</td>
				</tr>
			</table>
	
	<table align=center width=1000px>
				<tr>
					<td align=center width=250px>
						<center>
							<video width="240" height="140"  muted autoplay loop>
							 <source src="resources/sofa/images_low.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							 <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/sofa/images_IAT.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							  <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/sofa/images_LLVE.mp4" type="video/mp4">
							</video>
						</center>
					</td>
					<td align=center width=250px>
						<center>
							  <video width="240" height="140"  muted autoplay loop>
							 <source src="resources/sofa/images_aleth.mp4" type="video/mp4">
							</video>
						</center>
					</td>
				</tr>
			</table>
	
	<table align=center width=1000px>
		<br>
	<table align=center width=1000px>
		<tr>
			<td>
				[1]. You Only Need 90K Parameters to Adapt Light: A Light Weight Transformer for Image Enhancement and Exposure Correction (BMVC 2022)
				<br>
				[2]. Learning Temporal Consistency for Low Light Video Enhancement from Single Images (CVPR 2021)
			<td>
		<tr>
	<table align=center width=1000px>
	
	<br>
	
	<hr>
	<center><h1>Method</h1></center>
	<table align=center width=950px>
		<tr>
			<td>
			An overview of our method shown as follow, we design 2 types of Concealing Fields, the local concealing field and the global concealing field.
			</td>
		</tr>
	</table>
	<br/>
	
	<table align=center width=650px>
		<tr>
			<td align=center width=650px>
				<center>
					<td><img class="round" style="width:650px" src="resources/structure.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br/>
	<table align=center width=950px>
		<tr>
			<td>
			Train on adverse lighting condition images C<sup>adv</sup>, Aleth-NeRF performs unsupervised lightness correction by 
			(a). remove concealing fields in low-light conditions and (b). add concealing fields in over-exposure conditions.
			</td>
		</tr>
	</table>
	<br/>
	
	<table align=center width=500px>
		<tr>
			<td align=center width=500px>
				<center>
					<td><img class="round" style="width:500px" src="resources/Corr.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br/>
	<table align=center width=950px>
		<tr>
			<td>
			Along camera ray r (z axis), concealing fields and density σ exhibit a negative correlation, This validates that concealing fields are separated from density, thus rarely participating in scene rendering. 
			Concealing fields exists more in locations r(i) with sparse density, i.e. air outside the objects..
			</td>
		</tr>
	</table>
	<br/>
	
	
	<hr>

	<center><h1>LOM Dataset</h1></center>
	<table align=center width=700px>
		<tr>
			<td align=center width=700px>
				<center>
					<td><img class="round" style="width:700px" src="resources/dataset.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br/>
	
	<table align=center width=950px>
		<tr>
			<td>
			We collect the first paired Low-light & Over-exposure & normal-light Multi-view dataset, <strong>LOM</strong> dataset. Including 5 scenes: <i>buu</i>, <i>chair</i>, 
				<i>sofa</i>, <i>bike</i> and <i>shrub</i>.  Download the <strong>LOM</strong> dataset from: <a href="https://drive.google.com/file/d/1Fhe4UZ4aIgiMHiVzizvvqWukLkefygmL/view">[google drive]</a> or 
				<a href="https://pan.baidu.com/s/1Jx6wqbLvBVSHJRLNQYARlA">[baiduyun (passwd: cbhr)]</a>. 
			</td>
		</tr>
	</table>
	<br/>
	
	<hr>

	<center><h1>Experimental Results</h1></center>
	
	<table align=center width=1000px>
		<tr>
			<td align=center width=1000px>
				<center>
					<td><img class="round" style="width:1000px" src="resources/results.png"/></td>
				</center>
			</td>
		</tr>
	</table>
			
	
	<br>
	<hr>
	<table align=center width=800px>
		<center><h1>Our Related Research</h1></center>
		<tr>
			<td style="padding:20px;width:50%;vertical-align:middle">
              <img src="resources/IAT.png" alt="IAT" width="380" height="160">
            </td>
			
	      <td width="50%" valign="center">
	      <a>Ziteng Cui, Kunchang Li, Lin Gu, Shenghan Su et.al.</a>
              <br>
              <br/>
	      <a>You Only Need 90K Parameters to Adapt Light: a Light Weight Transformer for Image Enhancement and Exposure Correction.</a>
              <br>
	      <br/>
              <a><strong>BMVC 2022</strong> (<a href="https://arxiv.org/abs/2205.14871">ArXiv</a>, <a href="https://github.com/cuiziteng/Illumination-Adaptive-Transformer">Github</a>).</a>
              <br>
              </td>
		</tr>
		
	      <tr>
			<td style="padding:20px;width:50%;vertical-align:middle">
              <img src="resources/MAET.png" alt="MAET" width="380" height="160">
            </td>
			
	      <td width="50%" valign="center">
	      <a>Ziteng Cui, Guo-Jun Qi, Lin Gu, Shaodi You et.al.</a>
              <br>
              <br/>
	      <a>Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection.</a>
              <br>
	      <br/>
              <a><strong>ICCV 2021</strong> (<a href="https://arxiv.org/abs/2205.03346">ArXiv</a>, <a href="https://github.com/cuiziteng/ICCV_MAET">Github</a>).</a>
              <br>
              </td>
		</tr>
		
	</table>
	<br>
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

